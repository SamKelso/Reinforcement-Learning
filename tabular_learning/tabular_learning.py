# -*- coding: utf-8 -*-
"""coursework_redo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B22lshz_ftVV4bLfNLg4NwvDteAXF-1Z
"""

import numpy as np
import random
import matplotlib.pyplot as plt # Graphical library
from sklearn.metrics import mean_squared_error # Mean-squared error function

"""# Coursework 1 :
See pdf for instructions.
"""

# WARNING: fill in these two functions that will be used by the auto-marking script
# [Action required]

def get_CID():
  return "01506756" # Return your CID (add 0 at the beginning to ensure it is 8 digits long)

def get_login():
  return "smk22" # Return your short imperial login

"""## Helper class"""

# This class is used ONLY for graphics
# YOU DO NOT NEED to understand it to work on this coursework

class GraphicsMaze(object):

  def __init__(self, shape, locations, default_reward, obstacle_locs, absorbing_locs, absorbing_rewards, absorbing):

    self.shape = shape
    self.locations = locations
    self.absorbing = absorbing

    # Walls
    self.walls = np.zeros(self.shape)
    for ob in obstacle_locs:
      self.walls[ob] = 20

    # Rewards
    self.rewarders = np.ones(self.shape) * default_reward
    for i, rew in enumerate(absorbing_locs):
      self.rewarders[rew] = 10 if absorbing_rewards[i] > 0 else -10

    # Print the map to show it
    self.paint_maps()

  def paint_maps(self):
    """
    Print the Maze topology (obstacles, absorbing states and rewards)
    input: /
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders)
    plt.show()

  def paint_state(self, state):
    """
    Print one state on the Maze topology (obstacles, absorbing states and rewards)
    input: /
    output: /
    """
    states = np.zeros(self.shape)
    states[state] = 30
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders + states)
    plt.show()

  def draw_deterministic_policy(self, Policy):
    """
    Draw a deterministic policy
    input: Policy {np.array} -- policy to draw (should be an array of values between 0 and 3 (actions))
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze
    for state, action in enumerate(Policy):
      if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action
        continue
      arrows = [r"$\uparrow$",r"$\rightarrow$", r"$\downarrow$", r"$\leftarrow$"] # List of arrows corresponding to each possible action
      action_arrow = arrows[action] # Take the corresponding action
      location = self.locations[state] # Compute its location on graph
      plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph
    plt.show()

  def draw_policy(self, Policy):
    """
    Draw a policy (draw an arrow in the most probable direction)
    input: Policy {np.array} -- policy to draw as probability
    output: /
    """
    deterministic_policy = np.array([np.argmax(Policy[row,:]) for row in range(Policy.shape[0])])
    self.draw_deterministic_policy(deterministic_policy)

  def draw_value(self, Value):
    """
    Draw a policy value
    input: Value {np.array} -- policy values to draw
    output: /
    """
    plt.figure(figsize=(15,10))
    plt.imshow(self.walls + self.rewarders) # Create the graph of the Maze
    for state, value in enumerate(Value):
      if(self.absorbing[0, state]): # If it is an absorbing state, don't plot any value
        continue
      location = self.locations[state] # Compute the value location on graph
      plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph
    plt.show()

  def draw_deterministic_policy_grid(self, Policies, title, n_columns, n_lines):
    """
    Draw a grid representing multiple deterministic policies
    input: Policies {np.array of np.array} -- array of policies to draw (each should be an array of values between 0 and 3 (actions))
    output: /
    """
    plt.figure(figsize=(20,8))
    for subplot in range (len(Policies)): # Go through all policies
      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy
      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze
      for state, action in enumerate(Policies[subplot]):
        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action
          continue
        arrows = [r"$\uparrow$",r"$\rightarrow$", r"$\downarrow$", r"$\leftarrow$"] # List of arrows corresponding to each possible action
        action_arrow = arrows[action] # Take the corresponding action
        location = self.locations[state] # Compute its location on graph
        plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph
      ax.title.set_text(title[subplot]) # Set the title for the graph given as argument
    plt.show()

  def draw_policy_grid(self, Policies, title, n_columns, n_lines):
    """
    Draw a grid representing multiple policies (draw an arrow in the most probable direction)
    input: Policy {np.array} -- array of policies to draw as probability
    output: /
    """
    deterministic_policies = np.array([[np.argmax(Policy[row,:]) for row in range(Policy.shape[0])] for Policy in Policies])
    self.draw_deterministic_policy_grid(deterministic_policies, title, n_columns, n_lines)

  def draw_value_grid(self, Values, title, n_columns, n_lines):
    """
    Draw a grid representing multiple policy values
    input: Values {np.array of np.array} -- array of policy values to draw
    output: /
    """
    plt.figure(figsize=(20,8))
    for subplot in range (len(Values)): # Go through all values
      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value
      ax.imshow(self.walls+self.rewarders) # Create the graph of the Maze
      for state, value in enumerate(Values[subplot]):
        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value
          continue
        location = self.locations[state] # Compute the value location on graph
        plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph
      ax.title.set_text(title[subplot]) # Set the title for the graoh given as argument
    plt.show()

"""## Maze class"""

# This class define the Maze environment

class Maze(object):

  # [Action required]
  def __init__(self):
    """
    Maze initialisation.
    input: /
    output: /
    """

    # [Action required]
    # Properties set from the CID
    self._prob_success = 0.88 # float
    self._gamma = 0.9 # float
    self._goal = 2 # integer (0 for R0, 1 for R1, 2 for R2, 3 for R3)

    # Build the maze
    self._build_maze()


  # Functions used to build the Maze environment
  # You DO NOT NEED to modify them
  def _build_maze(self):
    """
    Maze initialisation.
    input: /
    output: /
    """

    # Properties of the maze
    self._shape = (13, 10)
    self._obstacle_locs = [
                          (1,0), (1,1), (1,2), (1,3), (1,4), (1,7), (1,8), (1,9), \
                          (2,1), (2,2), (2,3), (2,7), \
                          (3,1), (3,2), (3,3), (3,7), \
                          (4,1), (4,7), \
                          (5,1), (5,7), \
                          (6,5), (6,6), (6,7), \
                          (8,0), \
                          (9,0), (9,1), (9,2), (9,6), (9,7), (9,8), (9,9), \
                          (10,0)
                         ] # Location of obstacles
    self._absorbing_locs = [(2,0), (2,9), (10,1), (12,9)] # Location of absorbing states
    self._absorbing_rewards = [ (500 if (i == self._goal) else -50) for i in range (4) ]
    self._starting_locs = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9)] #Reward of absorbing states
    self._default_reward = -1 # Reward for each action performs in the environment
    self._max_t = 500 # Max number of steps in the environment

    # Actions
    self._action_size = 4
    self._direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on

    # States
    self._locations = []
    for i in range (self._shape[0]):
      for j in range (self._shape[1]):
        loc = (i,j)
        # Adding the state to locations if it is no obstacle
        if self._is_location(loc):
          self._locations.append(loc)
    self._state_size = len(self._locations)

    # Neighbours - each line is a state, ranked by state-number, each column is a direction (N, E, S, W)
    self._neighbours = np.zeros((self._state_size, 4))

    for state in range(self._state_size):
      loc = self._get_loc_from_state(state)

      # North
      neighbour = (loc[0]-1, loc[1]) # North neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('N')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('N')] = state

      # East
      neighbour = (loc[0], loc[1]+1) # East neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('E')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('E')] = state

      # South
      neighbour = (loc[0]+1, loc[1]) # South neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('S')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('S')] = state

      # West
      neighbour = (loc[0], loc[1]-1) # West neighbours location
      if self._is_location(neighbour):
        self._neighbours[state][self._direction_names.index('W')] = self._get_state_from_loc(neighbour)
      else: # If there is no neighbour in this direction, coming back to current state
        self._neighbours[state][self._direction_names.index('W')] = state

    # Absorbing
    self._absorbing = np.zeros((1, self._state_size))
    for a in self._absorbing_locs:
      absorbing_state = self._get_state_from_loc(a)
      self._absorbing[0, absorbing_state] = 1

    # Transition matrix
    self._T = np.zeros((self._state_size, self._state_size, self._action_size)) # Empty matrix of domension S*S*A
    for action in range(self._action_size):
      for outcome in range(4): # For each direction (N, E, S, W)
        # The agent has prob_success probability to go in the correct direction
        if action == outcome:
          prob = 1 - 3.0 * ((1.0 - self._prob_success) / 3.0) # (theoritically equal to self.prob_success but avoid rounding error and garanty a sum of 1)
        # Equal probability to go into one of the other directions
        else:
          prob = (1.0 - self._prob_success) / 3.0

        # Write this probability in the transition matrix
        for prior_state in range(self._state_size):
          # If absorbing state, probability of 0 to go to any other states
          if not self._absorbing[0, prior_state]:
            post_state = self._neighbours[prior_state, outcome] # Post state number
            post_state = int(post_state) # Transform in integer to avoid error
            self._T[prior_state, post_state, action] += prob

    # Reward matrix
    self._R = np.ones((self._state_size, self._state_size, self._action_size)) # Matrix filled with 1
    self._R = self._default_reward * self._R # Set default_reward everywhere
    for i in range(len(self._absorbing_rewards)): # Set absorbing states rewards
      post_state = self._get_state_from_loc(self._absorbing_locs[i])
      self._R[:,post_state,:] = self._absorbing_rewards[i]

    # Creating the graphical Maze world
    self._graphics = GraphicsMaze(self._shape, self._locations, self._default_reward, self._obstacle_locs, self._absorbing_locs, self._absorbing_rewards, self._absorbing)

    # Reset the environment
    self.reset()


  def _is_location(self, loc):
    """
    Is the location a valid state (not out of Maze and not an obstacle)
    input: loc {tuple} -- location of the state
    output: _ {bool} -- is the location a valid state
    """
    if (loc[0] < 0 or loc[1] < 0 or loc[0] > self._shape[0]-1 or loc[1] > self._shape[1]-1):
      return False
    elif (loc in self._obstacle_locs):
      return False
    else:
      return True


  def _get_state_from_loc(self, loc):
    """
    Get the state number corresponding to a given location
    input: loc {tuple} -- location of the state
    output: index {int} -- corresponding state number
    """
    return self._locations.index(tuple(loc))


  def _get_loc_from_state(self, state):
    """
    Get the state number corresponding to a given location
    input: index {int} -- state number
    output: loc {tuple} -- corresponding location
    """
    return self._locations[state]

  # Getter functions used only for DP agents
  # You DO NOT NEED to modify them
  def get_T(self):
    return self._T

  def get_R(self):
    return self._R

  def get_absorbing(self):
    return self._absorbing

  # Getter functions used for DP, MC and TD agents
  # You DO NOT NEED to modify them
  def get_graphics(self):
    return self._graphics

  def get_action_size(self):
    return self._action_size

  def get_state_size(self):
    return self._state_size

  def get_gamma(self):
    return self._gamma

  # Functions used to perform episodes in the Maze environment
  def reset(self):
    """
    Reset the environment state to one of the possible starting states
    input: /
    output:
      - t {int} -- current timestep
      - state {int} -- current state of the envionment
      - reward {int} -- current reward
      - done {bool} -- True if reach a terminal state / 0 otherwise
    """
    self._t = 0
    self._state = self._get_state_from_loc(self._starting_locs[random.randrange(len(self._starting_locs))])
    self._reward = 0
    self._done = False
    return self._t, self._state, self._reward, self._done

  def step(self, action):
    """
    Perform an action in the environment
    input: action {int} -- action to perform
    output:
      - t {int} -- current timestep
      - state {int} -- current state of the envionment
      - reward {int} -- current reward
      - done {bool} -- True if reach a terminal state / 0 otherwise
    """

    # If environment already finished, print an error
    if self._done or self._absorbing[0, self._state]:
      print("Please reset the environment")
      return self._t, self._state, self._reward, self._done

    # Drawing a random number used for probaility of next state
    probability_success = random.uniform(0,1)

    # Look for the first possible next states (so get a reachable state even if probability_success = 0)
    new_state = 0
    while self._T[self._state, new_state, action] == 0:
      new_state += 1
    assert self._T[self._state, new_state, action] != 0, "Selected initial state should be probability 0, something might be wrong in the environment."

    # Find the first state for which probability of occurence matches the random value
    total_probability = self._T[self._state, new_state, action]
    while (total_probability < probability_success) and (new_state < self._state_size-1):
     new_state += 1
     total_probability += self._T[self._state, new_state, action]
    assert self._T[self._state, new_state, action] != 0, "Selected state should be probability 0, something might be wrong in the environment."

    # Setting new t, state, reward and done
    self._t += 1
    self._reward = self._R[self._state, new_state, action]
    self._done = self._absorbing[0, new_state] or self._t > self._max_t
    self._state = new_state
    return self._t, self._state, self._reward, self._done

"""## DP Agent"""

# This class define the Dynamic Programing agent

class DP_agent(object):

  # [Action required]
  # WARNING: make sure this function can be called by the auto-marking script
  def solve(self, env):
    """
    Solve a given Maze environment using Dynamic Programming
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - V {np.array} -- Corresponding value function
    """

    # Initialisation (can be edited)
    threshold = 0.001 # Set the threshold value.
    delta = 2*threshold # Setting value of delta to go through the first breaking condition
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    V = np.zeros(env.get_state_size())
    gamma = env.get_gamma()

    while(delta > threshold):
      delta = 0
      for state in np.arange(0, env.get_state_size()):
        v = np.copy(V[state]) # Store state value.
        action_values = np.zeros((1, env.get_action_size()))
        # Loop through possible actions to find best action.
        for action in range(0, env.get_action_size()):
          action_value = 0
          for next_state in np.arange(0, env.get_state_size()):
            action_value += (env.get_T()[state,next_state,action]*(env.get_R()[state,next_state,action] + gamma*V[next_state])) # Calculate value if this action is taken.
          action_values[:,action] = action_value
        max_action_index = np.argmax(action_values)
        new_action = np.zeros((1,env.get_action_size()))
        new_action[:,max_action_index] = 1
        policy[state,:] = new_action # Update the policy with the best action as we go.
        V[state] = action_values[:,max_action_index] # Update the value with the value of taking the best action.
        delta = max(delta,abs(v-V[state])) # Compute delta.

    return policy, V

"""## MC agent"""

# This class define the Monte-Carlo agent

class MC_agent(object):


  def epsilon_greedy(self, env, Q, policy, epsilon):
    """
    Function which finds the epsilon-greedy policy based on Q_hat.
    input:
      - Q {np.array (state_size, action_size)}: Current estimation of value function for each state and action.
      - epsilon {float}: value to apply epsilon greedy policy.
    output:
      - policy {np.array (state_size, action_size)}: epsilon greedy policy.
    """

    for state in Q.keys():
      new_state_policy = np.ones(env.get_action_size())*(epsilon/env.get_action_size())
      best_action_index = np.argmax(Q[state])
      new_state_policy[best_action_index] += (1.0 - epsilon)
      policy[state,:] = new_state_policy
    return policy

  def get_value_function(self, env, Q):
    """
    Function which gets the value function from our state action function and current policy.
    input:
      - Q {np.array (state_size, action_size)}: current Q function.
      - policy {np.array (state_size, action_size)}: current epsilon greedy policy
    output:
      - V {np.array (state_size,)}: value function.
    """
    V = np.zeros(env.get_state_size())
    for state in range(0,V.shape[0]):
      action = np.argmax(Q[state])
      V[state] = Q[state][action]
    return V

  # [Action required]
  # WARNING: make sure this function can be called by the auto-marking script
  def solve_eps_test(self, env, epsilon, episodes):
    """
    Solve a given Maze environment using Monte Carlo learning
    input:
      - env {Maze object} -- Maze to solve
      - epsilon
      -episodes

    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - values {list of np.array} -- List of successive value functions for each episode
      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
    """

    # Initialisation (can be edited)
    returns_sum = dict()
    returns_count = dict()
    Q = dict()
    Q_array = np.random.rand(env.get_state_size(), env.get_action_size())
    for state in range(Q_array.shape[0]):
      Q[state] = Q.setdefault(state, Q_array[state])
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    policy = self.epsilon_greedy(env, Q, policy, epsilon)


    V = np.zeros(env.get_state_size())
    values = [V]
    total_rewards = []

    action_choices = np.arange(env.get_action_size())

    for i in range(0, episodes):
      t, state, reward, done = env.reset()
      trace_ = dict()
      trace_states = []
      trace_actions = []
      trace_rewards = []
      while(done == 0):
        trace_states.append(state)
        action = np.random.choice(action_choices,p=policy[state])
        trace_actions.append(action)
        t, state, reward, done = env.step(action)
        trace_rewards.append(reward)
      for i in range(0,len(trace_states)):
        if(not (trace_states[i],trace_actions[i]) in trace_.keys()):
          reward = 0
          for j, reward_ in enumerate(trace_rewards[i:]):
            reward += reward_*(env.get_gamma()**j)
          trace_[(trace_states[i],trace_actions[i])] = reward
          if((trace_states[i],trace_actions[i]) in returns_sum.keys()):
            returns_sum[(trace_states[i],trace_actions[i])] += reward
            returns_count[(trace_states[i],trace_actions[i])] += 1
          else:
            returns_sum[(trace_states[i],trace_actions[i])] = reward
            returns_count[(trace_states[i],trace_actions[i])] = 1
      total_rewards.append(sum(trace_rewards))
      for state_action in trace_.keys():
        state_ = state_action[0]
        action = state_action[1]
        Q[state_][action] = returns_sum[state_action]/returns_count[state_action]
      policy = self.epsilon_greedy(env, Q, policy, epsilon)
      V = self.get_value_function(env, Q)
      values.append(V)
    for state_ in Q.keys():
      max_action_index = np.argmax(Q[state_])
      policy[state_] = np.zeros(env.get_action_size())
      policy[state_,max_action_index] = 1


    return policy, values, total_rewards

  def solve(self, env):
    """
    Solve a given Maze environment using Monte Carlo learning
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - values {list of np.array} -- List of successive value functions for each episode
      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
    """
    #Initialisation
    epsilon = 0.8
    returns_sum = dict()
    returns_count = dict()
    Q = dict()
    Q_array = np.random.rand(env.get_state_size(), env.get_action_size())
    for state in range(Q_array.shape[0]):
      Q[state] = Q.setdefault(state, Q_array[state])
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    policy = self.epsilon_greedy(env, Q, policy, epsilon)


    V = np.zeros(env.get_state_size())
    values = [V]
    total_rewards = []

    episodes = 4000

    action_choices = np.arange(env.get_action_size())

    for i in range(0, episodes):
      t, state, reward, done = env.reset()
      trace_ = dict()
      trace_states = []
      trace_actions = []
      trace_rewards = []
      while(done == 0):
        trace_states.append(state)
        action = np.random.choice(action_choices,p=policy[state])
        trace_actions.append(action)
        t, state, reward, done = env.step(action)
        trace_rewards.append(reward)
      for i in range(0,len(trace_states)):
        if(not (trace_states[i],trace_actions[i]) in trace_.keys()):
          reward = 0
          for j, reward_ in enumerate(trace_rewards[i:]):
            reward += reward_*(env.get_gamma()**j)
          trace_[(trace_states[i],trace_actions[i])] = reward
          if((trace_states[i],trace_actions[i]) in returns_sum.keys()):
            returns_sum[(trace_states[i],trace_actions[i])] += reward
            returns_count[(trace_states[i],trace_actions[i])] += 1
          else:
            returns_sum[(trace_states[i],trace_actions[i])] = reward
            returns_count[(trace_states[i],trace_actions[i])] = 1
      total_rewards.append(sum(trace_rewards))
      for state_action in trace_.keys():
        state_ = state_action[0]
        action = state_action[1]
        Q[state_][action] = returns_sum[state_action]/returns_count[state_action]
      policy = self.epsilon_greedy(env, Q, policy, epsilon)
      V = self.get_value_function(env, Q)
      values.append(V)
    for state_ in Q.keys():
      max_action_index = np.argmax(Q[state_])
      policy[state_] = np.zeros(env.get_action_size())
      policy[state_,max_action_index] = 1


    return policy, values, total_rewards


  def solve_average_visit(self, env, epsilon):
    """
    Solve a given Maze environment using Monte Carlo learning
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - values {list of np.array} -- List of successive value functions for each episode
      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
    """

    # Initialisation (can be edited)
    #epsilon = 0.5
    returns_sum = dict()
    returns_count = dict()
    Q = dict()
    Q_array = np.random.rand(env.get_state_size(), env.get_action_size())
    for state in range(Q_array.shape[0]):
      Q[state] = Q.setdefault(state, Q_array[state])
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    policy = self.epsilon_greedy(env, Q, policy, epsilon)


    V = np.zeros(env.get_state_size())
    values = [V]
    total_rewards = []

    episodes = 200

    action_choices = np.arange(env.get_action_size())

    for i in range(0, episodes):
      t, state, reward, done = env.reset()
      trace_ = dict()
      trace_states = []
      trace_actions = []
      trace_rewards = []
      while(done == 0):
        trace_states.append(state)
        action = np.random.choice(action_choices,p=policy[state])
        trace_actions.append(action)
        t, state, reward, done = env.step(action)
        trace_rewards.append(reward)
      for i in range(0,len(trace_states)):
        #if(not (trace_states[i],trace_actions[i]) in trace_.keys()):
        reward = 0
        for j, reward_ in enumerate(trace_rewards[i:]):
          reward += reward_*(env.get_gamma()**j)
        trace_[(trace_states[i],trace_actions[i])] = reward
        if((trace_states[i],trace_actions[i]) in returns_sum.keys()):
          returns_sum[(trace_states[i],trace_actions[i])] += reward
          returns_count[(trace_states[i],trace_actions[i])] += 1
        else:
          returns_sum[(trace_states[i],trace_actions[i])] = reward
          returns_count[(trace_states[i],trace_actions[i])] = 1
      total_rewards.append(sum(trace_rewards))
      for state_action in trace_.keys():
        state_ = state_action[0]
        action = state_action[1]
        Q[state_][action] = returns_sum[state_action]/returns_count[state_action]
      policy = self.epsilon_greedy(env, Q, policy, epsilon)
      V = self.get_value_function(env, Q)
      values.append(V)
    for state_ in Q.keys():
      max_action_index = np.argmax(Q[state_])
      policy[state_] = np.zeros(env.get_action_size())
      policy[state_,max_action_index] = 1


    return policy, values, total_rewards


"""## TD agent"""

# This class define the Temporal-Difference agent

class TD_agent(object):


  def epsilon_greedy_state_policy(self, env, Q, state, epsilon):
    best_action_index = np.argmax(Q[state])
    state_policy = np.ones(env.get_action_size())*(epsilon/env.get_action_size())
    state_policy[best_action_index] += (1.0 - epsilon)
    return state_policy

  def epsilon_greedy(self, env, Q, policy, epsilon):
    """
    Function which finds the epsilon-greedy policy based on Q_hat.
    input:
      - Q {np.array (state_size, action_size)}: Current estimation of value function for each state and action.
      - epsilon {float}: value to apply epsilon greedy policy.
    output:
      - policy {np.array (state_size, action_size)}: epsilon greedy policy.
    """

    for state in Q.keys():
      new_state_policy = np.ones(env.get_action_size())*(epsilon/env.get_action_size())
      best_action_index = np.argmax(Q[state])
      new_state_policy[best_action_index] += (1.0 - epsilon)
      policy[state,:] = new_state_policy
    return policy

  def get_value_function(self, env, Q):
    """
    Function which gets the value function from our state action function and current policy.
    input:
      - Q {np.array (state_size, action_size)}: current Q function.
      - policy {np.array (state_size, action_size)}: current epsilon greedy policy
    output:
      - V {np.array (state_size,)}: value function.
    """
    V = np.zeros(env.get_state_size())
    for state in range(0,V.shape[0]):
      action = np.argmax(Q[state])
      V[state] = Q[state][action]
    return V

  # [Action required]
  # WARNING: make sure this function can be called by the auto-marking script

  def solve_q_learning(self, env, epsilon, alpha, episodes):
    """
    Solve a given Maze environment using Temporal Difference learning
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - values {list of np.array} -- List of successive value functions for each episode
      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
    """

    # Initialisation (can be edited)
    #epsilon = 0.8
    #alpha = 0.4
    returns_sum = dict()
    returns_count = dict()
    Q = dict()
    Q_array = np.random.rand(env.get_state_size(), env.get_action_size())
    for state in range(Q_array.shape[0]):
      Q[state] = Q.setdefault(state, Q_array[state])
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    policy = self.epsilon_greedy(env, Q, policy, epsilon)


    V = np.zeros(env.get_state_size())
    values = [V]
    total_rewards = []

    #episodes = 7000

    action_choices = np.arange(env.get_action_size())

    for i in range(0, episodes):
      t, state, reward, done = env.reset()
      state_policy = self.epsilon_greedy_state_policy(env, Q, state, epsilon)
      action = np.random.choice(action_choices,p=state_policy)
      total_reward = 0
      while(done == 0):
        #Take action.
        #t, next_state, reward, done = env.step(action)
        state_policy = self.epsilon_greedy_state_policy(env, Q, state, epsilon)
        action = np.random.choice(action_choices,p=state_policy)
        t, next_state, reward, done = env.step(action)
        Q[state][action] = Q[state][action] + alpha*(reward + env.get_gamma()*(np.amax(Q[next_state])) - Q[state][action])
        state = next_state
        total_reward += reward
      V = self.get_value_function(env, Q)
      values.append(V)
      total_rewards.append(total_reward)
    for state_ in Q.keys():
      max_action_index = np.argmax(Q[state_])
      policy[state_] = np.zeros(env.get_action_size())
      policy[state_,max_action_index] = 1

    return policy, values, total_rewards

  def solve(self, env):
    """
    Solve a given Maze environment using Temporal Difference learning
    input: env {Maze object} -- Maze to solve
    output:
      - policy {np.array} -- Optimal policy found to solve the given Maze environment
      - values {list of np.array} -- List of successive value functions for each episode
      - total_rewards {list of float} -- Corresponding list of successive total non-discounted sum of reward for each episode
    """

    # Initialisation (can be edited)
    epsilon = 0.7
    alpha = 0.2
    returns_sum = dict()
    returns_count = dict()
    Q = dict()
    Q_array = np.random.rand(env.get_state_size(), env.get_action_size())
    for state in range(Q_array.shape[0]):
      Q[state] = Q.setdefault(state, Q_array[state])
    policy = np.zeros((env.get_state_size(), env.get_action_size()))
    policy = self.epsilon_greedy(env, Q, policy, epsilon)


    V = np.zeros(env.get_state_size())
    values = [V]
    total_rewards = []

    episodes = 10000

    action_choices = np.arange(env.get_action_size())

    for i in range(0, episodes):
      t, state, reward, done = env.reset()
      state_policy = self.epsilon_greedy_state_policy(env, Q, state, epsilon)
      action = np.random.choice(action_choices,p=state_policy)
      total_reward = 0
      while(done == 0):
        #Take action.
        #t, next_state, reward, done = env.step(action)
        state_policy = self.epsilon_greedy_state_policy(env, Q, state, epsilon)
        action = np.random.choice(action_choices,p=state_policy)
        t, next_state, reward, done = env.step(action)
        Q[state][action] = Q[state][action] + alpha*(reward + env.get_gamma()*(np.amax(Q[next_state])) - Q[state][action])
        state = next_state
        total_reward += reward
      V = self.get_value_function(env, Q)
      values.append(V)
      total_rewards.append(total_reward)
    for state_ in Q.keys():
      max_action_index = np.argmax(Q[state_])
      policy[state_] = np.zeros(env.get_action_size())
      policy[state_,max_action_index] = 1

    return policy, values, total_rewards

if __name__ == '__main__':
    # DP plots.

    print("Creating the Grid world:\n")
    grid = Maze()

    dp_agent = DP_agent()
    policy, V = dp_agent.solve(grid)

    #Draw graphical representation of optimal value and policy/
    print("The graphical representation of the value of the optimal policy computed using value iteration is:")
    grid.get_graphics().draw_value(V)
    print("The graphical representation of the optimal policy computed using value iteration is:")
    grid.get_graphics().draw_policy(policy)


    # MC plots.

    print("Creating the Grid world:\n")
    grid = Maze()

    mc_agent = MC_agent()
    grid.reset()

    # Print final output.
    for epsilon in [0.8]:
      policy, values, total_rewards = mc_agent.solve_eps_test(grid, epsilon, 4000)
      print(f"The graphical representation of the value of the optimal policy computed using value iteration is, epsilon = {epsilon}:")
      grid.get_graphics().draw_value(values[-1])
      print(f"The graphical representation of the optimal policy computed using value iteration is, epsilon = {epsilon}:")
      grid.get_graphics().draw_policy(policy)

    # Output variability plots.
    total_rewards_list = []
    values_list = []
    for k in range(10,71,20):
      print(k)
      total_rewards_list = []
      values_list = []
      for i in range(0,k):
        policy, values, total_rewards = mc_agent.solve_eps_test(grid, 0.8, 100)
        total_rewards_list.append(total_rewards)
        values_list.append(values)
      values_list = np.array(values_list)
      values_mean = np.zeros(values_list.shape[1])
      for i in range(0, values_list.shape[1]):
        sum_squares = 0
        for j in range(0, values_list.shape[2]):
          sum_squares += np.square(np.mean(values_list[:,i,j]))
        values_mean[i] = np.sqrt(sum_squares)
      episode_num = 101
      episodes = np.arange(episode_num)
      plt.figure(figsize=(4,3))
      plt.plot(episodes, values_mean, color = "green")
      plt.xlabel("episodes")
      plt.ylabel("value function metric")
      plt.show()

    # Output varying epsilon learning curves.
    import statistics
    for epsilon in [0.2, 0.4, 0.6, 0.8]:
      print(epsilon)
      total_rewards_list = []
      #values_list = []
      for i in range(0,50):
        policy, values, total_rewards = mc_agent.solve_eps_test(grid, epsilon, 1000)
        total_rewards_list.append(total_rewards)
      total_rewards_list = np.array(total_rewards_list)
      total_rewards_mean = np.zeros(total_rewards_list.shape[1])
      total_rewards_std = np.zeros(total_rewards_list.shape[1])
      for i in range(0, total_rewards_list.shape[1]):
        total_rewards_mean[i] = np.mean(total_rewards_list[:,i])
        total_rewards_std[i] = statistics.stdev(total_rewards_list[:,i])
      episode_num = 1000
      episodes = np.arange(episode_num)
      plt.figure(figsize=(10,5))
    #plt.plot(episodes,total_rewards[:episode_num])
      plt.plot(episodes, total_rewards_mean, color = "blue", label="total reward mean")
      plt.fill_between(episodes, total_rewards_mean-total_rewards_std, total_rewards_mean+total_rewards_std, color = 'cornflowerblue', label="total reward standard deviation")
      #plt.plot(episodes, total_rewards_std, color = "red", label="total reward standard deviation")
      plt.xlabel("Episodes")
      plt.ylabel("Total reward")
      plt.legend(loc="lower right")
      plt.show()

    # Output optimal value learning curve.
    import statistics
    for epsilon in [0.8]:
      print(epsilon)
      total_rewards_list = []
      #values_list = []
      for i in range(0,50):
        policy, values, total_rewards = mc_agent.solve_eps_test(grid, epsilon, 4000)
        total_rewards_list.append(total_rewards)
      total_rewards_list = np.array(total_rewards_list)
      total_rewards_mean = np.zeros(total_rewards_list.shape[1])
      total_rewards_std = np.zeros(total_rewards_list.shape[1])
      for i in range(0, total_rewards_list.shape[1]):
        total_rewards_mean[i] = np.mean(total_rewards_list[:,i])
        total_rewards_std[i] = statistics.stdev(total_rewards_list[:,i])
      episode_num = 4000
      episodes = np.arange(episode_num)
      plt.figure(figsize=(10,5))
    #plt.plot(episodes,total_rewards[:episode_num])
      plt.plot(episodes, total_rewards_mean, color = "blue", label="total reward mean")
      plt.fill_between(episodes, total_rewards_mean-total_rewards_std, total_rewards_mean+total_rewards_std, color = 'cornflowerblue', label="total reward standard deviation")
      #plt.plot(episodes, total_rewards_std, color = "red", label="total reward standard deviation")
      plt.xlabel("Episodes")
      plt.ylabel("Total reward")
      plt.legend(loc= "lower right")
      plt.show()

    # Optimal learning curve 500 episodes.
    import statistics
    for epsilon in [0.8]:
      print(epsilon)
      total_rewards_list = []
      #values_list = []
      for i in range(0,50):
        policy, values, total_rewards = mc_agent.solve_eps_test(grid, epsilon, 500)
        total_rewards_list.append(total_rewards)
      total_rewards_list = np.array(total_rewards_list)
      total_rewards_mean = np.zeros(total_rewards_list.shape[1])
      total_rewards_std = np.zeros(total_rewards_list.shape[1])
      for i in range(0, total_rewards_list.shape[1]):
        total_rewards_mean[i] = np.mean(total_rewards_list[:,i])
        total_rewards_std[i] = statistics.stdev(total_rewards_list[:,i])
      episode_num = 500
      episodes = np.arange(episode_num)
      plt.figure(figsize=(10,5))
    #plt.plot(episodes,total_rewards[:episode_num])
      plt.plot(episodes, total_rewards_mean, color = "blue", label="total reward mean")
      plt.fill_between(episodes, total_rewards_mean-total_rewards_std, total_rewards_mean+total_rewards_std, color = 'cornflowerblue', label="total reward standard deviation")
      #plt.plot(episodes, total_rewards_std, color = "red", label="total reward standard deviation")
      plt.xlabel("Episodes")
      plt.ylabel("Total reward")
      plt.legend(loc= "lower right")
      plt.show()

    #TD plots.

    td_agent = TD_agent()
    grid.reset()

    policy, values, total_rewards = td_agent.solve_q_learning(grid,0.7,0.2,10000)

    print("The graphical representation of the value of the optimal policy computed using value iteration is:")
    grid.get_graphics().draw_value(values[-1])
    print("The graphical representation of the optimal policy computed using value iteration is:")
    grid.get_graphics().draw_policy(policy)

    """Epsilon comparisons"""

    # Plots for epsilon comparisons.
    import statistics
    for epsilon in [0.2, 0.4, 0.6, 0.8]:
      print(epsilon)
      total_rewards_list = []
      #values_list = []
      for i in range(0,50):
        policy, values, total_rewards = td_agent.solve_q_learning(grid, epsilon, 0.2, 1000)
        total_rewards_list.append(total_rewards)
      total_rewards_list = np.array(total_rewards_list)
      total_rewards_mean = np.zeros(total_rewards_list.shape[1])
      total_rewards_std = np.zeros(total_rewards_list.shape[1])
      for i in range(0, total_rewards_list.shape[1]):
        total_rewards_mean[i] = np.mean(total_rewards_list[:,i])
        total_rewards_std[i] = statistics.stdev(total_rewards_list[:,i])
      episode_num = 1000
      episodes = np.arange(episode_num)
      plt.figure(figsize=(10,5))
    #plt.plot(episodes,total_rewards[:episode_num])
      plt.plot(episodes, total_rewards_mean, color = "blue", label="total reward mean")
      plt.fill_between(episodes, total_rewards_mean-total_rewards_std, total_rewards_mean+total_rewards_std, color = 'cornflowerblue', label="total reward standard deviation")
      #plt.plot(episodes, total_rewards_std, color = "red", label="total reward standard deviation")
      plt.xlabel("Episodes")
      plt.ylabel("Total reward")
      plt.legend(loc="lower right")
      plt.show()

    """Alpha Comparisons"""

    # Plot for alpha comparisons.
    import statistics
    alpha_means = []
    alpha_stds = []
    for alpha in [0.01, 0.1, 0.2, 0.5, 0.9]:
      #print(epsilon)
      total_rewards_list = []
      #values_list = []
      for i in range(0,50):
        policy, values, total_rewards = td_agent.solve_q_learning(grid, 0.5, alpha, 200)
        total_rewards_list.append(total_rewards)
      total_rewards_list = np.array(total_rewards_list)
      total_rewards_mean = np.zeros(total_rewards_list.shape[1])
      total_rewards_std = np.zeros(total_rewards_list.shape[1])
      for i in range(0, total_rewards_list.shape[1]):
        total_rewards_mean[i] = np.mean(total_rewards_list[:,i])
        total_rewards_std[i] = statistics.stdev(total_rewards_list[:,i])
      alpha_means.append(total_rewards_mean)
      alpha_stds.append(total_rewards_std)

    episode_num = 200
    episodes = np.arange(episode_num)
    alphas = [0.01,0.1,0.2,0.5,0.9]
    plt.figure(figsize=(10,5))
    for i in range(0, len(alpha_means)):
      #episode_num = 200
      #episodes = np.arange(episode_num)
      #plt.figure(figsize=(10,5))
      plt.plot(episodes, alpha_means[i], label=f"alpha = {alphas[i]}")
    plt.legend(loc="lower right")
    plt.xlabel("Episodes")
    plt.ylabel("Total Reward")
    plt.show()

    """Learning Curve"""

    # Plot for learning curve
    import statistics
    for alpha in [0.2]:
      print(epsilon)
      total_rewards_list = []
      #values_list = []
      for i in range(0,50):
        policy, values, total_rewards = td_agent.solve_q_learning(grid, 0.7, alpha, 10000)
        total_rewards_list.append(total_rewards)
      total_rewards_list = np.array(total_rewards_list)
      total_rewards_mean = np.zeros(total_rewards_list.shape[1])
      total_rewards_std = np.zeros(total_rewards_list.shape[1])
      for i in range(0, total_rewards_list.shape[1]):
        total_rewards_mean[i] = np.mean(total_rewards_list[:,i])
        total_rewards_std[i] = statistics.stdev(total_rewards_list[:,i])
      episode_num = 10000
      episodes = np.arange(episode_num)
      plt.figure(figsize=(10,5))
    #plt.plot(episodes,total_rewards[:episode_num])
      plt.plot(episodes, total_rewards_mean, color = "blue", label="total reward mean")
      plt.fill_between(episodes, total_rewards_mean-total_rewards_std, total_rewards_mean+total_rewards_std, color = 'cornflowerblue', label="total reward standard deviation")
      #plt.plot(episodes, total_rewards_std, color = "red", label="total reward standard deviation")
      plt.xlabel("Episodes")
      plt.ylabel("Total reward")
      plt.legend(loc="lower right")
      plt.show()

    # Plot for learing curve.
    import statistics
    for alpha in [0.2]:
      print(epsilon)
      total_rewards_list = []
      #values_list = []
      for i in range(0,50):
        policy, values, total_rewards = td_agent.solve_q_learning(grid, 0.7, alpha, 500)
        total_rewards_list.append(total_rewards)
      total_rewards_list = np.array(total_rewards_list)
      total_rewards_mean = np.zeros(total_rewards_list.shape[1])
      total_rewards_std = np.zeros(total_rewards_list.shape[1])
      for i in range(0, total_rewards_list.shape[1]):
        total_rewards_mean[i] = np.mean(total_rewards_list[:,i])
        total_rewards_std[i] = statistics.stdev(total_rewards_list[:,i])
      episode_num = 500
      episodes = np.arange(episode_num)
      plt.figure(figsize=(10,5))
    #plt.plot(episodes,total_rewards[:episode_num])
      plt.plot(episodes, total_rewards_mean, color = "blue", label="total reward mean")
      plt.fill_between(episodes, total_rewards_mean-total_rewards_std, total_rewards_mean+total_rewards_std, color = 'cornflowerblue', label="total reward standard deviation")
      #plt.plot(episodes, total_rewards_std, color = "red", label="total reward standard deviation")
      plt.xlabel("Episodes")
      plt.ylabel("Total reward")
      plt.legend(loc="lower right")
      plt.show()

    total_rewards[-1]

    """# Comparison of learners #"""

    print("Creating the Grid world:\n")
    grid = Maze()

    # Run dp.
    dp_agent = DP_agent()
    dp_policy, dp_V = dp_agent.solve(grid)
    len(dp_V)

    #MC_50_reps
    mc_agent = MC_agent()
    mc_values_list = []
    for i in range(0,50):
      mc_policy, mc_values, mc_total_rewards = mc_agent.solve_eps_test(grid, 0.8, 4000)
      mc_values_list.append(mc_values)

    mc_values_list = np.array(mc_values_list)
    mc_values_list.shape

    mc_values_list = np.array(mc_values_list)
    mc_mse = np.zeros((mc_values_list.shape[0],mc_values_list.shape[1]))
    mc_mse_mean = np.zeros(mc_values_list.shape[1])
    mc_mse_std = np.zeros(mc_values_list.shape[1])
    for i in range(0, mc_values_list.shape[0]):
      for j in range(0, mc_values_list.shape[1]):
        mc_mse[i,j] = mean_squared_error(dp_V, mc_values_list[i,j,:])

    for k in range(mc_mse.shape[1]):
      mc_mse_mean[k] = np.mean(mc_mse[:,k])
      mc_mse_std[k] = statistics.stdev(mc_mse[:,k])

    #TD_50_reps
    td_agent = TD_agent()
    td_values_list = []
    for i in range(0,50):
      td_policy, td_values, td_total_rewards = td_agent.solve_q_learning(grid, 0.7, 0.2, 4000)
      td_values_list.append(td_values)

    td_values_list = np.array(td_values_list)
    td_mse = np.zeros((td_values_list.shape[0],td_values_list.shape[1]))
    td_mse_mean = np.zeros(td_values_list.shape[1])
    td_mse_std = np.zeros(td_values_list.shape[1])
    for i in range(0, td_values_list.shape[0]):
      for j in range(0, td_values_list.shape[1]):
        td_mse[i,j] = mean_squared_error(dp_V, td_values_list[i,j,:])

    for k in range(mc_mse.shape[1]):
      td_mse_mean[k] = np.mean(td_mse[:,k])
      td_mse_std[k] = statistics.stdev(td_mse[:,k])

    episodes = np.arange(4001)
    plt.figure(figsize=(8,5))
    plt.plot(episodes,mc_mse_mean, color = "blue", label = "MC")
    plt.fill_between(episodes, mc_mse_mean-mc_mse_std, mc_mse_mean+mc_mse_std, color = 'cornflowerblue', alpha=0.3, label = "MC standard deviation")
    plt.plot(episodes,td_mse_mean, color = "red", label = "TD")
    plt.fill_between(episodes, td_mse_mean-td_mse_std, td_mse_mean+td_mse_std, color = 'lightcoral',alpha=0.3, label="TD standard deviation")
    plt.xlabel("episodes")
    plt.ylabel("mse")
    plt.legend(loc="upper right")
    plt.show()

    mc_agent = MC_agent()
    mc_policy, mc_values, mc_total_rewards = mc_agent.solve_eps_test(grid, 0.4, 4000)
    len(mc_values)

    td_agent = TD_agent()
    td_policy, td_values, td_total_rewards = td_agent.solve_q_learning(grid, 0.4, 0.2, 10000)
    len(td_values[-1])

    #MC
    mse_mc = []
    for i in range(0,len(mc_values)):
      mse_mc_episode = mean_squared_error(dp_V, mc_values[i])
      mse_mc.append(mse_mc_episode)

    #TD
    mse_td = []
    for i in range(0,len(td_values)):
      mse_td_episode = mean_squared_error(dp_V, td_values[i])
      mse_td.append(mse_td_episode)

    plt.figure(figsize=(10,10))
    plt.scatter(mc_total_rewards, mse_mc[:-1])
    plt.xlabel("total_reward")
    plt.ylabel("mse")
    plt.show()

    plt.figure(figsize=(10,10))
    plt.scatter(td_total_rewards, mse_td[:-1])
    plt.xlabel("total_reward")
    plt.ylabel("mse")
    plt.show()

    """## Example main"""

    # Example main (can be edited)

    ### Question 0: Defining the environment

    print("Creating the Maze:\n")
    maze = Maze()


    ### Question 1: Dynamic programming

    dp_agent = DP_agent()
    dp_policy, dp_value = dp_agent.solve(maze)

    print("Results of the DP agent:\n")
    maze.get_graphics().draw_policy(dp_policy)
    maze.get_graphics().draw_value(dp_value)


    ### Question 2: Monte-Carlo learning

    mc_agent = MC_agent()
    mc_policy, mc_values, total_rewards = mc_agent.solve(maze)

    print("Results of the MC agent:\n")
    maze.get_graphics().draw_policy(mc_policy)
    maze.get_graphics().draw_value(mc_values[-1])


    ### Question 3: Temporal-Difference learning

    td_agent = TD_agent()
    td_policy, td_values, total_rewards = td_agent.solve(maze)

    print("Results of the TD agent:\n")
    maze.get_graphics().draw_policy(td_policy)
    maze.get_graphics().draw_value(td_values[-1])
